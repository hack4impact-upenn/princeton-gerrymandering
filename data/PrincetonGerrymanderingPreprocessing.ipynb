{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PrincetonGerrymanderingPreprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hack4impact/princeton-gerrymandering/blob/master/data/PrincetonGerrymanderingPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y27nmHLL3rYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install docx2txt\n",
        "!pip install boto3\n",
        "!pip install spacy\n",
        "!pip install elasticsearch\n",
        "!apt-get install poppler-utils \n",
        "!pip install pdf2image\n",
        "!pip instrall pytesseract\n",
        "!mkdir tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LZeO8tccywk",
        "colab_type": "code",
        "outputId": "a6ae9e0f-169f-42cd-ed22-bd40282d03e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import boto3\n",
        "import docx2txt\n",
        "import spacy \n",
        "import os \n",
        "import operator\n",
        "\n",
        "from elasticsearch import Elasticsearch\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "\n",
        "  \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "\n",
        "# Connect to elasticsearch\n",
        "es = Elasticsearch(['https://search-pg-project-54kjhgwd4bxzxkbuh6cvw2hdsq.us-east-1.es.amazonaws.com/'])\n",
        "\n",
        "if not es.ping():\n",
        "  raise ValueError(\"Connection to ElasticSearch failed\")\n",
        "  sys.exit(1)\n",
        "else:\n",
        "  print('Connection to ElasticSearch OK')\n",
        "\n",
        "# Connect to s3 bucket with test data\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIAVJ3TAJ6AADY5E2JW',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "\n",
        "s3 = session.resource('s3')\n",
        "bucket = s3.Bucket(\"princeton-gerrymandering\")\n",
        "\n",
        "\n",
        "# sorts tags in the order of how many initially appeared, keep duplicates for elasticsearch weighting\n",
        "def clean_tags(tags):\n",
        "  for key in tags:\n",
        "    count_dict = {}\n",
        "    for tag in tags[key]:\n",
        "      if tag in count_dict:\n",
        "        count_dict[tag] += 1\n",
        "      else:\n",
        "        count_dict[tag] = 1\n",
        "    processed_tags = sorted(tags[key], key=lambda x : count_dict[x], reverse=True)\n",
        "    tags[key] = processed_tags\n",
        "\n",
        "\n",
        "# Aggregates spacy tags into tags we care about\n",
        "def aggregate_spacy_tags(tag):\n",
        "  spacy_tags = {\n",
        "      \"PERSON\": \"people\",\n",
        "      \"NORP\": \"orgs\",\n",
        "      \"FAC\": \"locations\",\n",
        "      \"ORG\": \"orgs\",\n",
        "      \"GPE\": \"locations\",\n",
        "      \"LOC\": \"locations\",\n",
        "      \"PRODUCT\": \"other\",\n",
        "      \"EVENT\": \"events\",\n",
        "      \"WORK_OF_ART\": \"other\",\n",
        "      \"LAW\": \"documents\",\n",
        "      \"DATE\": \"datetime\",\n",
        "      \"TIME\": \"datetime\",\n",
        "      \"PERCENT\": \"other\",\n",
        "      \"MONEY\": \"money\",\n",
        "      \"ORDINAL\": \"other\",\n",
        "      \"CARDINAL\": \"other\",\n",
        "      \"QUANTITY\": \"other\",\n",
        "      \"LANGUAGE\": \"other\"\n",
        "    }\n",
        "  if tag not in spacy_tags: \n",
        "    return \"other\"\n",
        "  else:\n",
        "    return spacy_tags[tag]\n",
        "\n",
        "def default_json(aws_key):\n",
        "  return {\n",
        "      \"path\": aws_key,\n",
        "      \"name\": aws_key.split(\"/\")[-1],\n",
        "      \"filetype\": \"\",\n",
        "      \"tags\": {\n",
        "          \"people\": [],\n",
        "          \"orgs\": [],\n",
        "          \"locations\": [],\n",
        "          \"datetime\": [],\n",
        "          \"documents\": [],\n",
        "          \"events\": [],\n",
        "          \"money\": [], \n",
        "          \"other\": []\n",
        "      },\n",
        "      \"text\": \"\"\n",
        "  }\n",
        "\n",
        "def handle_pdf(local_filename, aws_key):\n",
        "  json_res = default_json(aws_key)\n",
        "  pdf_text = \"\"\n",
        "  pages = convert_from_path(pdf_path=local_filename)\n",
        "  for page in pages:\n",
        "      pdf_text += \" \" + pytesseract.image_to_string(page)\n",
        "  doc = nlp(pdf_text)\n",
        "  for ent in doc.ents:\n",
        "    tag = aggregate_spacy_tags(ent.label_)\n",
        "    if tag in json_res[\"tags\"]:\n",
        "      json_res[\"tags\"][tag].append(ent.text)\n",
        "    else:\n",
        "      json_res[\"tags\"][tag] = [ent.text]\n",
        "  clean_tags(json_res[\"tags\"])\n",
        "  json_res[\"text\"] = pdf_text\n",
        "  json_res[\"filetype\"] = \"pdf\"\n",
        "  return json_res\n",
        "  \n",
        "\n",
        "# generate JSON file for .docx files\n",
        "def handle_docx(local_filename, aws_key):\n",
        "  text = docx2txt.process(local_filename)\n",
        "  json_res = default_json(aws_key)\n",
        "  doc = nlp(text)\n",
        "  for ent in doc.ents:\n",
        "    tag = aggregate_spacy_tags(ent.label_)\n",
        "    if tag in json_res[\"tags\"]:\n",
        "      json_res[\"tags\"][tag].append(ent.text)\n",
        "    else:\n",
        "      json_res[\"tags\"][tag] = [ent.text]\n",
        "  clean_tags(json_res[\"tags\"])\n",
        "  json_res[\"text\"] = text\n",
        "  json_res[\"filetype\"] = \"docx\"\n",
        "  return json_res\n",
        "\n",
        "def handle_txt(local_filename, aws_key):\n",
        "  f = open(local_filename, \"r\", encoding = \"ISO-8859-1\")\n",
        "  text = f.read()\n",
        "  json_res = default_json(aws_key)\n",
        "  doc = nlp(text)\n",
        "  for ent in doc.ents:\n",
        "    tag = aggregate_spacy_tags(ent.label_)\n",
        "    if tag in json_res[\"tags\"]:\n",
        "      json_res[\"tags\"][tag].append(ent.text)\n",
        "    else:\n",
        "      json_res[\"tags\"][tag] = [ent.text]\n",
        "  clean_tags(json_res[\"tags\"])\n",
        "  json_res[\"text\"] = text\n",
        "  json_res[\"filetype\"] = \"txt\"\n",
        "  return json_res\n",
        "    \n",
        "def handle_fallback(local_filename, aws_key):\n",
        "  return default_json(aws_key)\n",
        "\n",
        "\n",
        "# iterate through all files in bucket, parse them accordingly\n",
        "unsupported_exts = []\n",
        "for o in bucket.objects.all():\n",
        "  file_ext = o.key.split(\".\")[-1].lower()\n",
        "  local_filename = './tmp/%s.%s' % (o.e_tag.strip(\"\\\"\"), file_ext)\n",
        "  s3.meta.client.download_file('princeton-gerrymandering', o.key, local_filename)\n",
        "\n",
        "  json_doc = None\n",
        "\n",
        "\n",
        "  if file_ext == 'docx':\n",
        "    json_doc = handle_docx(local_filename, o.key)\n",
        "  elif file_ext == 'pdf':\n",
        "    json_doc = handle_pdf(local_filename, o.key)\n",
        "  # elif file_ext == 'xlsx':\n",
        "  #   json_doc = handle_xlsx(local_filename, o.key)\n",
        "  elif file_ext == 'txt' or file_ext == 'log' :\n",
        "    json_doc = handle_txt(local_filename, o.key)\n",
        "    # print(json_doc)\n",
        "  else:\n",
        "    json_doc = handle_fallback(local_filename, o.key)\n",
        "    if file_ext not in unsupported_exts:\n",
        "      unsupported_exts.append(file_ext)\n",
        "  res = es.index(index=\"pgp\", body=json_doc)\n",
        "  os.remove(local_filename)\n",
        "\n",
        "print (\"Unsupported File Formats: \")\n",
        "print(unsupported_exts)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Connection to ElasticSearch OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu8dOoPfYBMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade pymupdf"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}